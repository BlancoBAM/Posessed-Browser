---
title: "Local Models"
description: "Run AI entirely on your machine with Lilim, Ollama, or LM Studio"
---

# Local Models

Posessed Browser is built to run AI locally. No cloud dependency, no subscription, no data leaving your machine.

## Lilim (Recommended on Lilith Linux)

[Lilim](https://github.com/BlancoBAM/Lilim) is the purpose-built AI for Lilith Linux and Posessed Browser.

- **Lilith Linux users:** Lilim is pre-installed and pre-configured. Nothing to do.
- **Other systems:** Install from [github.com/BlancoBAM/Lilim](https://github.com/BlancoBAM/Lilim)

Lilim is optimized for the Posessed Browser environment and will grow more capable with each Lilith Linux release.

## Ollama

Run open-source LLMs locally for free.

```bash
# Install Ollama (Linux)
curl -fsSL https://ollama.com/install.sh | sh

# Pull a model
ollama pull llama3          # General purpose
ollama pull mistral-nemo    # Fast, good for agents
ollama pull gemma2          # Google's open model
```

Then in Posessed Browser: **Settings → AI Provider → Ollama**

## LM Studio

Download from [lmstudio.ai](https://lmstudio.ai), choose a model from the built-in browser, start the local server, then connect in **Settings → AI Provider → LM Studio**.

## Choosing a Local Model

| Use Case                | Recommended                    |
| ----------------------- | ------------------------------ |
| Lilith Linux (any task) | Lilim                          |
| Chat & summarization    | Any model works                |
| Agent / automation      | Lilim or `mistral-nemo`        |
| Coding                  | `codellama` or `qwen2.5-coder` |

<Warning>
  For complex Agent Mode tasks (multi-step automation, form filling, data
  extraction), most local models other than Lilim may struggle. Lilim is
  specifically designed for this environment.
</Warning>
